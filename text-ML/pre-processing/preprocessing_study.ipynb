{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dc72e4",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1e903",
   "metadata": {},
   "source": [
    "## 1 - Tokenizar (Biblioteca nltk)\n",
    "\n",
    "### 1.1 - Sentence Tokenization (Tokenizar Sentenças)\n",
    "\n",
    "- 1.1.1 - Sent Tokenize: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5942079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Phoenix Suns e o Miami Heat estrearam com vitória em suas semifinais de Conferência nos playoffs da NBA.', 'Com atuação mais convincente, os Suns lembraram os momentos de regularidade na temporada, quando dominaram!', 'O jogo ocorreu na noite desta segunda-feira (2).']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "texto = \"Phoenix Suns e o Miami Heat estrearam com vitória em suas semifinais de Conferência nos playoffs da NBA. Com atuação mais convincente, os Suns lembraram os momentos de regularidade na temporada, quando dominaram! O jogo ocorreu na noite desta segunda-feira (2).\"\n",
    "\n",
    "st = nltk.sent_tokenize\n",
    "tokens = st(text=texto, language='portuguese')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468ee62",
   "metadata": {},
   "source": [
    "- 1.1.2 - RegexpTokenizer: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf3ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Phoenix Suns e o Miami Heat estrearam com vitória em suas semifinais de Conferência nos playoffs da NBA. Com atuação mais convincente, os Suns lembraram os momentos de regularidade na temporada, quando dominaram!', 'O jogo ocorreu na noite desta segunda-feira (2).']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "texto = \"Phoenix Suns e o Miami Heat estrearam com vitória em suas semifinais de Conferência nos playoffs da NBA. Com atuação mais convincente, os Suns lembraram os momentos de regularidade na temporada, quando dominaram! O jogo ocorreu na noite desta segunda-feira (2).\"\n",
    "\n",
    "rt = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "tokens = rt.tokenize(texto)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606215b7",
   "metadata": {},
   "source": [
    "### 1.2 - Word Tokenization (Tokenizar Palavras)\n",
    "\n",
    "- 1.2.1 - Word Tokenize:<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4ab96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'Toronto', 'Raptors', 'é', 'um', 'time', 'de', 'basquete', 'profissional', 'canadense', 'sediado', 'em', 'Toronto', ',', 'Ontário']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentenca = \"O Toronto Raptors é um time de basquete profissional canadense sediado em Toronto, Ontário\"\n",
    "\n",
    "wt = nltk.word_tokenize \n",
    "tokens = wt(sentenca)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eecd260",
   "metadata": {},
   "source": [
    "- 1.2.2 - RegexpTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0b412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'Toronto', 'Raptors', 'é', 'um', 'time', 'de', 'basquete', 'profissional', 'canadense', 'sediado', 'em', 'Toronto', 'Ontário']\n",
      "['O', 'Toronto', 'Raptors', 'é', 'um', 'time', 'de', 'basquete', 'profissional', 'canadense', 'sediado', 'em', 'Toronto,', 'Ontário']\n",
      "[(0, 1), (2, 9), (10, 17), (18, 19), (20, 22), (23, 27), (28, 30), (31, 39), (40, 52), (53, 62), (63, 70), (71, 73), (74, 82), (83, 90)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentenca = \"O Toronto Raptors é um time de basquete profissional canadense sediado em Toronto, Ontário\"\n",
    "\n",
    "# No Gap (só palavras)\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "wt_nogap = nltk.tokenize.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False) \n",
    "tokens_nogap = wt_nogap.tokenize(sentenca)\n",
    "print(tokens_nogap)\n",
    "\n",
    "# Gap (pontuação - \"palavra completa\")\n",
    "GAP_PATTERN = r'\\s+'\n",
    "wt_nogap = nltk.tokenize.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True) \n",
    "tokens_gap = wt_nogap.tokenize(sentenca)\n",
    "print(tokens_gap)\n",
    "\n",
    "# Mostrar os índices dos tokens\n",
    "indices = list(wt_nogap.span_tokenize(sentenca))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88f57a",
   "metadata": {},
   "source": [
    "- 1.2.3 - Outras Funções (Punctuation e White Space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9cabeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'Toronto', 'Raptors', 'é', 'um', 'time', 'de', 'basquete', 'profissional', 'canadense', 'sediado', 'em', 'Toronto', ',', 'Ontário']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Punctuation - Separa pontuação\n",
    "\n",
    "sentenca = \"O Toronto Raptors é um time de basquete profissional canadense sediado em Toronto, Ontário\"\n",
    "\n",
    "wt_punct = nltk.WordPunctTokenizer()\n",
    "tokens = wt_punct.tokenize(sentenca)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4612a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'Toronto', 'Raptors', 'é', 'um', 'time', 'de', 'basquete', 'profissional', 'canadense', 'sediado', 'em', 'Toronto,', 'Ontário']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# WhiteSpace - Separa Espaço\n",
    "\n",
    "sentenca = \"O Toronto Raptors é um time de basquete profissional canadense sediado em Toronto, Ontário\"\n",
    "\n",
    "wt_punct = nltk.WhitespaceTokenizer()\n",
    "tokens = wt_punct.tokenize(sentenca)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ae70c",
   "metadata": {},
   "source": [
    "## 2 - Normalização\n",
    "\n",
    "### 2.1 - Cleaning Text\n",
    "\n",
    "- 2.1.1 - Removendo Caracteres Especiais: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4a8cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']], [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'], ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']], [['@', '@', 'You', \"'ll\", '(', 'learn', ')', 'a', '*', '*', 'lot', '*', '*', 'in', 'the', 'book', '.'], ['Python', 'is', 'an', 'amazing', 'language', '!'], ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re \n",
    "import string \n",
    "from pprint import pprint \n",
    "\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\", \n",
    " \"Hey that's a great deal! I just bought a phone for $199\", \n",
    " \"@@You'll (learn) a **lot** in the book. Python is an amazing language !@@\"] \n",
    "\n",
    "# Função para tokenização\n",
    "def tokenize_text(text): \n",
    "    sentences = nltk.sent_tokenize(text) \n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
    "    return word_tokens\n",
    "\n",
    "# Tokenização do corpus\n",
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d570c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remoção simplificada:\n",
      " ['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language ']\n",
      "\n",
      "Remoção com Regex, sem apostrófos:\n",
      " ['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language ']\n",
      "\n",
      "Remoção com Regex, com apostrófos:\n",
      " [\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language !\"]\n"
     ]
    }
   ],
   "source": [
    "# Função de Remoção depois da tokenização\n",
    "def remove_characters_after_tokenization(tokens): \n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) \n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens]) \n",
    "    return filtered_tokens \n",
    "\n",
    "filtra_depois_tokenizacao = [filter(None, [remove_characters_after_tokenization(tokens) for tokens in sentence_tokens]) \n",
    "                             for sentence_tokens in token_list]\n",
    "\n",
    "# Remoção antes da tokenização\n",
    "pattern = re.compile('[{}]'.format(string.punctuation))\n",
    "corpus_nopunct = [pattern.sub('', sentence) for sentence in corpus]\n",
    "print(\"Remoção simplificada:\\n\", corpus_nopunct)\n",
    "\n",
    "# Função remoção antes da tokenização com Regex\n",
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False): \n",
    "    sentence = sentence.strip() \n",
    "    if keep_apostrophes: \n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # outros caracteres que serão removidos\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else: \n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # Somente letras e números\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "filtra_sem_apost = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print(\"\\nRemoção com Regex, sem apostrófos:\\n\", filtra_sem_apost)\n",
    "\n",
    "filtra_com_apost = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
    "print(\"\\nRemoção com Regex, com apostrófos:\\n\", filtra_com_apost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629f653",
   "metadata": {},
   "source": [
    "- 2.1.2 - Expandindo contração: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f3bed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa-se a seguinte função com CONTRATION_MAP sendo um dicionário com as keys sendo as contrações e os values a sua forma expandida\n",
    "\n",
    "\"\"\"\n",
    "from contractions import CONTRACTION_MAP \n",
    "def expand_contractions(sentence, contraction_mapping): \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) \n",
    "    def expand_match(contraction): \n",
    "        match = contraction.group(0) \n",
    "        first_char = match[0] \n",
    "        expanded_contraction = contraction_mapping.get(match)\\ \n",
    "                                if contraction_mapping.get(match)\\ \n",
    "                                else contraction_mapping.get(match.lower()) \n",
    "        expanded_contraction = first_char+expanded_contraction[1:] \n",
    "        return expanded_contraction \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n",
    "    return expanded_sentence\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887cef4",
   "metadata": {},
   "source": [
    "- 2.1.3 - Case Conversion: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa45865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversão para letras minúsculas:\n",
      " the brown fox wasn't that quick and he couldn't win the race\n",
      "\n",
      "Conversão para letras maiúsculas:\n",
      " THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "print(\"Conversão para letras minúsculas:\\n\", corpus[0].lower())\n",
    "print(\"\\nConversão para letras maiúsculas:\\n\", corpus[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be452006",
   "metadata": {},
   "source": [
    "### 2.2 - StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60565e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The', 'brown', 'fox', 'wasnt', 'quick', 'couldnt', 'win', 'race']],\n",
       " [['Hey', 'thats', 'great', 'deal', 'I', 'bought', 'phone', '199']],\n",
       " [['Youll', 'learn', 'lot', 'book', 'Python', 'amazing', 'language']]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Função que limpa as stopwords\n",
    "\n",
    "def remove_stopwords(tokens, lang='english'): \n",
    "    stopword_list = nltk.corpus.stopwords.words(lang) \n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list] \n",
    "    return filtered_tokens\n",
    "\n",
    "# Tokenização do corpus limpo\n",
    "token_list_sw = [tokenize_text(text) for text in corpus_nopunct]\n",
    "\n",
    "# Aplicação função limpar stopwords\n",
    "no_stopwords = [[remove_stopwords(tokens) for tokens in sentence_tokens] \n",
    "                for sentence_tokens in token_list_sw]\n",
    "\n",
    "no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb57b6b",
   "metadata": {},
   "source": [
    "### 2.3 - Correção ortográfica\n",
    "\n",
    "- 2.3.1 - Removendo Repetição de Caracteres: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15188d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem a função de remoção de caracteres repetidos:\n",
      " ['The', 'NBA', 'is', 'a', 'professsssional', 'basketttttballl', 'leaaaaague', '.']\n",
      "\n",
      "Tokens com a função aplicada:\n",
      " ['The', 'NBA', 'is', 'a', 'professional', 'basketbal', 'league', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet \n",
    "import re\n",
    "\n",
    "# Função que verifica a repetição de \n",
    "def remove_repeated_characters(tokens): \n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)') \n",
    "    match_substitution = r'\\1\\2\\3' \n",
    "    \n",
    "    #Função que troca a palavra caso ela esteja no banco de palavras\n",
    "    def replace(old_word): \n",
    "        if wordnet.synsets(old_word): \n",
    "            return old_word \n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word) \n",
    "        return replace(new_word) if new_word != old_word else new_word \n",
    "    \n",
    "    correct_tokens = [replace(word) for word in tokens] \n",
    "    return correct_tokens\n",
    "\n",
    "teste_repeticao = 'The NBA is a professsssional basketttttballl leaaaaague.'\n",
    "tokens_repeticao = tokenize_text(teste_repeticao)[0]\n",
    "print(\"Tokens sem a função de remoção de caracteres repetidos:\\n\", tokens_repeticao)\n",
    "print(\"\\nTokens com a função aplicada:\\n\", remove_repeated_characters(tokens_repeticao))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bbd26",
   "metadata": {},
   "source": [
    "- 2.3.1 - Corrigir Escritas Erradas: <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo \"manual\"\n",
    "\n",
    "\"\"\"\n",
    "import re, collections \n",
    "\n",
    "# Get all words from the corpus \n",
    "def tokens(text): \n",
    "    return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "WORDS = tokens(file('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "# Return all strings that are zero edits away from the input word (i.e., the word itself). \n",
    "def edits0(word): \n",
    "    return {word}\n",
    "\n",
    "# Return all strings that are one edit away from the input word. \n",
    "def edits1(word): \n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz' \n",
    "    \n",
    "    # Return a list of all possible (first, rest) pairs that the input word is made of\n",
    "    def splits(word): \n",
    "        return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    \n",
    "    pairs = splits(word)\n",
    "    deletes = [a+b[1:] for (a, b) in pairs if b] \n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1] \n",
    "    replaces = [a+c+b[1:] for (a, b) in pairs for c in alphabet if b] \n",
    "    inserts = [a+c+b for (a, b) in pairs for c in alphabet] \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# Return all strings that are two edits away from the input word.\n",
    "def edits2(word):\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)} \n",
    "\n",
    "#  Return the subset of words that are actually in our WORD_COUNTS dictionary. \n",
    "def known(words): \n",
    "    return {w for w in words if w in WORD_COUNTS}\n",
    "    \n",
    "candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word]) \n",
    "\n",
    "# Spell-correct word in match, and preserve proper upper/lower/title case. \n",
    "def correct_match(match): \n",
    "    word = match.group()\n",
    "    \n",
    "    # Return the case-function appropriate for text: upper, lower, title, or just str.:\n",
    "    def case_of(text): \n",
    "        return (str.upper if text.isupper() else \n",
    "                str.lower if text.islower() else \n",
    "                str.title if text.istitle() else \n",
    "                str) \n",
    "    return case_of(word)(correct(word.lower())) \n",
    "    \n",
    "# Correct all the words within a text, returning the corrected text. \n",
    "def correct_text_generic(text): \n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\"\"\"\n",
    "\n",
    "# Algoritmos prontos\n",
    "\n",
    "# Pattern -> Suggest\n",
    "# PyEnchant\n",
    "# aspell-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807df862",
   "metadata": {},
   "source": [
    "### 2.4 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e2e60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pul'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Um algoritmo pronto com suporte à português\n",
    "\n",
    "from nltk.stem import SnowballStemmer, RSLPStemmer\n",
    "ss = SnowballStemmer(\"portuguese\")\n",
    "ss.stem(\"pular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4c30",
   "metadata": {},
   "source": [
    "### 2.5 - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "561f57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "eat\n",
      "run\n",
      "sad\n",
      "fancy\n",
      "\n",
      "\n",
      "ate\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# Algoritmo pronto para inglês\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Nesse caso, é necessário falar se a palavra é um nome (n), verbo (v) ou advérbio (a)\n",
    "wnl = WordNetLemmatizer() \n",
    "\n",
    "# Funciona - classe correta\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))\n",
    "\n",
    "# Não funciona - classe incorreta\n",
    "print('\\n')\n",
    "print(wnl.lemmatize('ate', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5aa87",
   "metadata": {},
   "source": [
    "### 2.6 - Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a839de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O o DET X True\n",
      "Toronto Toronto PROPN Xxxxx False\n",
      "Raptors Raptors PROPN Xxxxx False\n",
      "é ser AUX x True\n",
      "um um DET xx True\n",
      "time time NOUN xxxx False\n",
      "de de ADP xx True\n",
      "basquete basquete NOUN xxxx False\n",
      "profissional profissional ADJ xxxx False\n",
      "canadense canadensar VERB xxxx False\n",
      "sediado sediar VERB xxxx False\n",
      "em em ADP xx True\n",
      "Toronto Toronto NOUN Xxxxx False\n",
      ". . PUNCT . False\n"
     ]
    }
   ],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download pt_core_news_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# É necessário entregar dados pré-treinados\n",
    "sp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "doc = sp('O Toronto Raptors é um time de basquete profissional canadense sediado em Toronto.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma_, token.pos_, token.shape_, token.is_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
